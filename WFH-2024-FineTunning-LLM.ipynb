{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYV9SMrqKxUs"
   },
   "source": [
    "<center><h2><strong><font color=\"blue\">WFH 2024 idBigData - FineTunning LLM</font></strong></h2></center>\n",
    "\n",
    "<img alt=\"\" src=\"https://github.com/taudataanalytics/WFH-idBigData-2024/blob/main/images/covers/cover_taudata_uin.jpg?raw=1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1olxKtS4KxUu"
   },
   "source": [
    "# Jangan lupa mengganti Runtime menjadi GPU di Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NBcX96rKxUv"
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"  # Disable wandb logging\n",
    "\n",
    "try:\n",
    "    import google.colab; IN_COLAB = True\n",
    "    print(\"Installing the required modules\")\n",
    "    !pip install datasets --q\n",
    "    print(\"preparing directories and assets\")\n",
    "    #!mkdir data images output models\n",
    "    #!wget https://raw.githubusercontent.com/taudata...\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"  # Disable wandb logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Step 3: Load pre-trained DistilGPT2 and tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token to avoid padding error\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set pad token ID in model config\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jfbJr1bKxUw"
   },
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Op49FrWKxUx"
   },
   "outputs": [],
   "source": [
    "# Step 4: Load a small dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Select a small subset of the data for quick training\n",
    "small_train_dataset = dataset[\"train\"].select(range(100))  # Select first 100 examples\n",
    "small_eval_dataset = dataset[\"validation\"].select(range(10))  # Select first 10 examples\n",
    "\n",
    "# Step 5: Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize and set 'labels' to 'input_ids' for supervised training\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcRfJXirKxUx"
   },
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnaPg41sKxUx"
   },
   "outputs": [],
   "source": [
    "# Step 6: Define training arguments (disable logging to any external services)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",  # Disable external logging\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,  # Adjust for more training\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Step 7: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMy8giiTKxUx"
   },
   "source": [
    "<center><h2><strong><font color=\"blue\">Train Model</font></strong></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "AgMzjZvRKxUy",
    "outputId": "5692e91f-77f6-425b-b4db-d3c5905cf317"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 00:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.324533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.292630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.285892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Artificial intelligence is transforming the way we interact with the world.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 9: Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine-tuned-distilgpt2\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-distilgpt2\")\n",
    "\n",
    "# Step 10: Test the model with text generation\n",
    "input_text = \"Artificial intelligence is transforming\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(device)\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],  # Pass attention mask explicitly\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.pad_token_id  # Set pad_token_id for reliable results\n",
    ")\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeoPuxAaURtZ"
   },
   "source": [
    "<center><h2><strong><font color=\"blue\">Keterangan</font></strong></h2></center>\n",
    "\n",
    "* **Dataset Loading**: We use a small subset of the WikiText dataset for simplicity. You can replace it with your own data if you wish.\n",
    "* **Tokenization**: The tokenizer converts text into token IDs compatible with DistilGPT2.\n",
    "* **Training**: The Trainer class handles training and evaluation with the specified parameters.\n",
    "* **Testing**: After fine-tuning, the model is tested on a short input to demonstrate its ability to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The code provided uses supervised fine-tuning, specifically leveraging the Trainer class from the Hugging Face transformers library to train a language model with labels derived from its input sequences. Here’s a breakdown of how it works and where it fits in the spectrum of fine-tuning approaches:\n",
    "\n",
    "1. Supervised Fine-Tuning with Labels\n",
    "* **Goal** : The model learns to predict the next token in the sequence based on supervised data.\n",
    "* **Labels** : In this code, the input_ids are set as labels in the tokenized dataset, essentially creating a teacher forcing setup where the model predicts each token given all previous tokens.\n",
    "* **Loss Calculation**: Since the labels are provided, the model calculates the cross-entropy loss during training, encouraging it to predict tokens correctly within the input context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Fine-Tuning Approach for Causal Language Models\n",
    "* **Sequential Prediction**: This method is typical for language models that generate text by predicting the next token, like GPT models. Each token prediction is based on previous tokens up to the current token.\n",
    "* **Training with Trainer**: The Trainer class simplifies the training loop, handling backpropagation, batching, and evaluation automatically. It’s a practical choice for demonstrations or cases where default training settings are sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Lain \n",
    "\n",
    "* **Instruction Tuning**: If we were tuning a model with specific instructions or response patterns (like ChatGPT), we might introduce special tokens to differentiate prompts from responses.\n",
    "* **Reinforcement Learning (RLHF)**: Reinforcement Learning from Human Feedback (RLHF) is another advanced fine-tuning method, particularly useful for aligning a model’s responses with human preferences, but it’s more complex, involving human feedback and reward models.\n",
    "* **Parameter-Efficient Tuning**: Techniques like LoRA (Low-Rank Adaptation) and PEFT (Parameter-Efficient Fine-Tuning) are alternatives that adapt specific parts of the model and are efficient for larger models or limited compute resources. These weren’t applied here but can be more suitable for low-resource environments."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
