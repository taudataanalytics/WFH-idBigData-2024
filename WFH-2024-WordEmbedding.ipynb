{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2><strong><font color=\"blue\">WFH 2024 idBigData - Word Embedding</font></strong></h2></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/covers/cover_taudata_uin.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><strong><font color=\"blue\">Referensi</font></strong></h2></center>\n",
    "\n",
    "* https://taudata.blogspot.com/2022/04/nlptm-05.html\n",
    "* https://www.youtube.com/watch?v=viZrOnJclY0\n",
    "* https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0\n",
    "* https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><strong><font color=\"blue\">One-Hot VS Word Embedding</font></strong></h1></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/BoW_VS_WordEmbedding.png\" style=\"height:250px;\" />\n",
    "\n",
    "<img alt=\"\" src=\"images/onehot-vs-embedding.png\" style=\"height:250px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><strong><font color=\"blue\">Word Embedding Models</font></strong></h1></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/DL/Word-Embedding-models.png\" style=\"height:400px;\" />\n",
    "\n",
    "* https://medium.com/nlplanet/two-minutes-nlp-11-word-embeddings-models-you-should-know-a0581763b9a9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/3_word2vec_example.png\" style=\"height:400px; width:667px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><strong><font color=\"blue\">Word2Vec</font></strong></h1></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/DL/Word2Vec.png\" style=\"height:200px;\" />\n",
    "\n",
    "* Dikembangkan oleh Tomas Mikolov - Google :</p>\n",
    "* Goldberg, Yoav; Levy, Omer. &quot;word2vec Explained: Deriving Mikolov et al.&#39;s Negative-Sampling Word-Embedding Method&quot;.&nbsp;<a href=\"https://en.wikipedia.org/wiki/ArXiv\">arXiv</a>:<a href=\"https://arxiv.org/abs/1402.3722\">1402.3722</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><strong><font color=\"blue\">FastText VS Word2Vec</font></strong></h1></center>\n",
    "\n",
    "<ul>\n",
    "\t<li>Menggunakan Sub-words: app, ppl, ple - apple</li>\n",
    "\t<li>Paper:&nbsp;https://arxiv.org/abs/1607.04606&nbsp;&nbsp;</li>\n",
    "\t<li>Website:&nbsp;https://fasttext.cc/</li>\n",
    "\t<li>Source:&nbsp;https://github.com/facebookresearch/fastText&nbsp;</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "<img alt=\"\" src=\"images/fastText-vs-Word2Vec.png\" style=\"height: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/DL/word2Vec-to-visualization-via-dimensional-reduction.png\" style=\"height:400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/3_cosine.png\" style=\"height:300px;\" />\n",
    "\n",
    "### Hati-hati Cosine adalah similarity bukan distance\n",
    "Hal ini akan mempengaruhi interpretasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m; warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm \n",
    "\n",
    "try:\n",
    "    import google.colab; IN_COLAB = True\n",
    "    print(\"Installing the required modules\")\n",
    "    !pip install torch, nltk, requests, dewiki, unidecode --q\n",
    "    print(\"preparing directories and assets\")\n",
    "    !mkdir data images output models\n",
    "    #!wget https://raw.githubusercontent.com/taudata...\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")\n",
    "\n",
    "import re, gc, os, nltk, requests\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import bz2,re,dewiki, html.parser\n",
    "from unidecode import unidecode\n",
    "from html import unescape\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "nSample = 100000 # Sample dokumen agar memory dan komputasi Tidak terlalu besar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing CPU and-or GPU ... \")\n",
    "gc.collect()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the {}:'.format(device.type.upper()), torch.cuda.get_device_name(0))\n",
    "    print(\"WARNING!!!... Walau menggunakan GPU Model ini tetap membutuhkan memori dan komputasi yang besar.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING!!!... No GPU available, using the '{}' instead. NOT Recommended, it is going to take forever for the code to finish!\".format(device.type.upper()))\n",
    "    print(\"Setidaknya gunakan sample Max 30.000 dokumen saja untuk pembelajaran.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Fungsi untuk membersihkan teks\n",
    "def cleanTags(text):\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # Remove HTML tags\n",
    "    text = re.sub(r'<\\?php.*?\\?>', '', text, flags=re.DOTALL)# Remove PHP tags\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)# Remove JavaScript\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL) # Remove CSS\n",
    "    return text\n",
    "\n",
    "def cleanText(text, minWord=2, maxWord=20):\n",
    "    t = dewiki.from_string(str(text))\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    t = re.sub(pattern,' ',t) #remove http urls if any\n",
    "    pattern = re.compile(r'ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    t = re.sub(pattern,' ',t) #remove ftp urls if any\n",
    "    t = re.sub(\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", t) #remove emails\n",
    "    t = cleanTags(t)\n",
    "    t = unescape(unidecode(t.lower().strip()))\n",
    "    t = t.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    #listKata = [tok for tok in TextBlob(t).words if (str(tok) not in stopWordsID)]\n",
    "    pattern = r\"[^a-zA-Z0-9 .,_-]\"\n",
    "    t = re.sub(pattern, \" \", t) # remove all Symbols except : {, . _ -}\n",
    "    t = re.sub(r'\\s+', ' ', t) # remove multiple spaces\n",
    "    t = \" \".join([w for w in t.split() if len(w)>=minWord and len(w)<=maxWord])\n",
    "    return t.strip()\n",
    "\n",
    "def getContents(dStr):\n",
    "    judul, pageID, body = None, None, None\n",
    "    if len(dStr)>7:\n",
    "        awalTag=re.search('<text.*?>',dStr).group(0)\n",
    "        awal=dStr.find(awalTag);akhir=dStr.find('</text>')\n",
    "        if awal>=0 and akhir>=0:\n",
    "            body=dStr[awal+len(awalTag):akhir]\n",
    "        dStr=dStr[:awal] # id and title came before <text>, no need to search the whole text again\n",
    "    if len(dStr)>7:\n",
    "        awal=dStr.find('<title>');akhir=dStr.find('</title>')\n",
    "        if awal>=0 and akhir>=0:\n",
    "            judul=dStr[awal+len('<title>'):akhir]\n",
    "        awal=dStr.find('<id>');akhir=dStr.find('</id>')\n",
    "        if awal>=0 and akhir>=0:\n",
    "            pageID=dStr[awal+len('<id>'):akhir]\n",
    "    return judul,pageID,body\n",
    "\n",
    "def getCategories(body):#if categories === if categories is not empty\n",
    "    categories=[];dTag1='[[Category:';dTag2=']]';n1=len(dTag1);n2=len(dTag2);\n",
    "    if type(body) is str:\n",
    "        while True and len(body)>12:\n",
    "            awal=body.find(dTag1);akhir=body.find(dTag2)\n",
    "            if awal>=0 and akhir>=0:\n",
    "                cat=body[awal+n1:akhir]\n",
    "                if len(cat)>1:\n",
    "                    categories.append(cat)\n",
    "                body=body[akhir+n2:]\n",
    "            else:\n",
    "                break\n",
    "    return ','.join(categories)\n",
    "\n",
    "# Buat pasangan konteks dan target\n",
    "def generate_context_target_pairs(tokens, context_size):\n",
    "    pairs = []\n",
    "    for token_list in tqdm(tokens):\n",
    "        token_list = [word for word in token_list if word in word_to_idx]\n",
    "        for i, word in enumerate(token_list):\n",
    "            target = word_to_idx[word]\n",
    "            context = [word_to_idx[token_list[i + j]] for j in range(-context_size, context_size + 1) if 0 <= i + j < len(token_list) and j != 0]\n",
    "            pairs.extend((context_word, target) for context_word in context)\n",
    "    return pairs\n",
    "\n",
    "# Dataset dan DataLoader\n",
    "class Word2VecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "    \n",
    "def get_word_embedding(word, GPU=True):\n",
    "    idx = word_to_idx.get(word)\n",
    "    if idx is not None:\n",
    "        with torch.no_grad():\n",
    "            if GPU:\n",
    "                embedding = model.embeddings(torch.tensor([idx]).cuda())\n",
    "            else:\n",
    "                embedding = model.embeddings(torch.tensor([idx]))\n",
    "        return torch.flatten(target_embedding).cpu()\n",
    "    else:\n",
    "        print(f\"Word '{word}' not in vocabulary\")\n",
    "        return None\n",
    "    \n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def find_most_similar(word, top_n=5, GPU=True):\n",
    "    target_embedding = get_word_embedding(word, GPU=GPU)\n",
    "    if target_embedding is None:\n",
    "        return\n",
    "\n",
    "    similarities = []\n",
    "    for other_word in tqdm(vocab):\n",
    "        if other_word != word:\n",
    "            other_embedding = get_word_embedding(other_word, GPU=GPU)\n",
    "            similarity = cosine_similarity(target_embedding, other_embedding)\n",
    "            similarities.append((other_word, similarity))\n",
    "    \n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Load Dataset Wikipedia Indonesia Latest Dump</font></center>\n",
    "\n",
    "* Keterangan tentang wiki dumps: https://en.wikipedia.org/wiki/Wikipedia:Database_download\n",
    "* Wikipedia Indonesia dumps: https://id.wikipedia.org/wiki/Wikipedia:Unduh_basis_data\n",
    "\n",
    "<img alt=\"\" src=\"images/Wiki_logo.jpeg\" style=\"height: 400px;\" />\n",
    "\n",
    "## <center><font color=\"red\">Hati-hati akan mendownload data Text yang cukup besar (Hampir 1Gb pada Juli 2024)</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unduh data Wikipedia bahasa Indonesia\n",
    "url = \"https://dumps.wikimedia.org/idwiki/latest/idwiki-latest-pages-articles.xml.bz2\"\n",
    "dataFile = \"data/idwiki-latest-pages-articles.xml.bz2\"\n",
    "\n",
    "if not os.path.isfile(dataFile) and not os.path.isfile(dataFile.replace(\".xml.bz2\", \".zip\")):\n",
    "    print(\"Start Downloading Wikipedia Dumps Data : '{}' ...\".format(dataFile), flush=True, end=\" \")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(dataFile, \"wb\") as file:\n",
    "        for chunk in tqdm(response.iter_content(chunk_size=1024)):\n",
    "            file.write(chunk)\n",
    "            \n",
    "if os.path.isfile(dataFile.replace(\".xml.bz2\", \".zip\")):\n",
    "    print(\"Reading the previously parsed Wikipedia XML dump: {}\".format(dataFile.replace(\".xml.bz2\", \".zip\")))\n",
    "    df = pd.read_csv(dataFile.replace(\".xml.bz2\", \".zip\"), compression='zip')\n",
    "else:\n",
    "    print(\"Reading the Wikipedia XML dump: {}\".format(dataFile))\n",
    "    print(\"Please wait, it might take awhile .... \")\n",
    "    id_,  dPage = 0, []\n",
    "    df = {\"id_\":[], \"category\":[], \"title\":[], \"content\":[]}\n",
    "    with bz2.BZ2File(dataFile, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            line=line.decode('UTF-8')\n",
    "            if '<page>' in line:\n",
    "                dPage=[]\n",
    "            elif '</page>' in line:\n",
    "                try:\n",
    "                    judul, _, body = getContents(' '.join(dPage))\n",
    "                    cat = cleanText(getCategories(body))\n",
    "                    judul = cleanText(judul)\n",
    "                    body = cleanText(body)\n",
    "                    if len(str(judul))>5 and len(str(body))>5:\n",
    "                        df['id_'].append(id_); id_ += 1\n",
    "                        df['category'].append(cat)\n",
    "                        df['title'].append(judul)\n",
    "                        df['content'].append(body)\n",
    "                except Exception as err_:\n",
    "                    #print(err_)\n",
    "                    pass\n",
    "            else:\n",
    "                dPage.append(line)\n",
    "    df = pd.DataFrame(df)\n",
    "    print(\"Finished, Compressing the dataFrame for future operations ... \")\n",
    "    df.to_csv(dataFile.replace(\".xml.bz2\", \".zip\"), index=False, compression={'method':'zip', 'compresslevel':9})\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sampling {} documents.\".format(nSample))\n",
    "df = df.sample(nSample)\n",
    "\n",
    "print(\"Joining Text ... \")\n",
    "texts = [\" \".join([t, c]) for t,c in tqdm(zip(df.title, df.content))]\n",
    "del df # Free some memory\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi teks\n",
    "print(\"Tokenizing ... \")\n",
    "tokens = [word_tokenize(text) for text in tqdm(texts)]\n",
    "del texts # Free Some memory\n",
    "\"\\nToken Length = {}\\n\".format(len(tokens)), tokens[0][:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Menyiapkan Struktur Data</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat korpus kata\n",
    "all_tokens = [token for sublist in tqdm(tokens) for token in sublist]\n",
    "vocab = Counter(all_tokens)\n",
    "vocab = [word for word, freq in vocab.items() if freq>=5 and len(word)>2]  # Filter kata yang jarang muncul\n",
    "\n",
    "# Peta kata ke indeks dan sebaliknya\n",
    "word_to_idx = {word: i for i, word in tqdm(enumerate(vocab))}\n",
    "idx_to_word = {i: word for i, word in tqdm(enumerate(vocab))}\n",
    "\n",
    "context_size = 2\n",
    "pairs = generate_context_target_pairs(tokens, context_size)\n",
    "\n",
    "dataset = Word2VecDataset(pairs)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab[:5])\n",
    "del all_tokens, tokens # free some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Model Word2Vec</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_word):\n",
    "        embeds = self.embeddings(context_word)\n",
    "        out = self.linear1(embeds)\n",
    "        return out\n",
    "\n",
    "embedding_dim = 50\n",
    "if device.type.lower().strip()==\"cpu\":\n",
    "    model = Word2Vec(vocab_size, embedding_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    model = Word2Vec(vocab_size, embedding_dim).cuda()\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Training</font></center>\n",
    "\n",
    "### Tambah epoch di kasus nyata dan gunakan early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpoch = 5 # Change to higher value in realworld scenario\n",
    "fmodel = 'data/models/word2Vec-wikiID.pth'\n",
    "try:\n",
    "    model = torch.load(fmodel)\n",
    "    print(\"Model Loaded from previous Run.\")\n",
    "except:\n",
    "    print(\"Training the model, please wait, it will take a while .. \")\n",
    "    for epoch in tqdm(range(nEpoch)):\n",
    "        total_loss = 0\n",
    "        for context_word, target in dataloader:\n",
    "            if device.type.lower().strip()==\"cpu\":\n",
    "                context_word, target = torch.tensor(context_word), torch.tensor(target)\n",
    "            else:\n",
    "                context_word, target = torch.tensor(context_word).cuda(), torch.tensor(target).cuda()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(context_word)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
    "    print(\"Finished!, Saving The Model... \")\n",
    "    torch.save(model, fmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Contoh Penggunaan</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type.lower().strip()==\"cpu\":\n",
    "    GPU = False\n",
    "else:\n",
    "    GPU = True\n",
    "word = 'agama'\n",
    "most_similar_words = find_most_similar(word, top_n=7, GPU=True)\n",
    "print(f\"Kata-kata yang paling mirip dengan '{word}':\")\n",
    "for similar_word, similarity in most_similar_words:\n",
    "    print(f\"{similar_word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Akhir Modul </font></center>\n",
    "\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/meme-cartoon/oprah-meme-everything-gets-a-vector-word-embedding.png\" style=\"height: 400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
