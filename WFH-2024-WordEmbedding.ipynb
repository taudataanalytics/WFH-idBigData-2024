{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2><strong><font color=\"blue\">WFH 2024 idBigData - Word Embedding</font></strong></h2></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/covers/cover_taudata_uin.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><strong><font color=\"blue\">Referensi</font></strong></h2></center>\n",
    "\n",
    "* https://taudata.blogspot.com/2022/04/nlptm-05.html\n",
    "* https://www.youtube.com/watch?v=viZrOnJclY0\n",
    "* https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0\n",
    "* https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/3_word2vec_example.png\" style=\"height:400px; width:667px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\taufi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "from tqdm import tqdm \n",
    "\n",
    "try:\n",
    "    import google.colab; IN_COLAB = True\n",
    "    print(\"Installing the required modules\")\n",
    "    !pip install torch, nltk, requests, dewiki, unidecode --q\n",
    "    print(\"preparing directories and assets\")\n",
    "    !mkdir data images output models\n",
    "    #!wget https://raw.githubusercontent.com/taudata...\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")\n",
    "\n",
    "import re, gc, os, nltk, requests\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import bz2,re,dewiki, html.parser\n",
    "from unidecode import unidecode\n",
    "from html import unescape\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "nSample = 100000 # Sample dokumen agar memory dan komputasi Tidak terlalu besar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing CPU and-or GPU ... \n",
      "There are 1 GPU(s) available.\n",
      "We will use the CUDA: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "WARNING!!!... Walau menggunakan GPU Model ini tetap membutuhkan memori dan komputasi yang besar.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing CPU and-or GPU ... \")\n",
    "gc.collect()\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256\"\n",
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the {}:'.format(device.type.upper()), torch.cuda.get_device_name(0))\n",
    "    print(\"WARNING!!!... Walau menggunakan GPU Model ini tetap membutuhkan memori dan komputasi yang besar.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING!!!... No GPU available, using the '{}' instead. NOT Recommended, it is going to take forever for the code to finish!\".format(device.type.upper()))\n",
    "    print(\"Setidaknya gunakan sample Max 30.000 dokumen saja untuk pembelajaran.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fungsi untuk membersihkan teks\n",
    "def cleanTags(text):\n",
    "    text = re.sub(r'<[^>]+>', ' ', text) # Remove HTML tags\n",
    "    text = re.sub(r'<\\?php.*?\\?>', '', text, flags=re.DOTALL)# Remove PHP tags\n",
    "    text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)# Remove JavaScript\n",
    "    text = re.sub(r'<style.*?>.*?</style>', '', text, flags=re.DOTALL) # Remove CSS\n",
    "    return text\n",
    "\n",
    "def cleanText(text, minWord=2, maxWord=20):\n",
    "    t = dewiki.from_string(str(text))\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    t = re.sub(pattern,' ',t) #remove http urls if any\n",
    "    pattern = re.compile(r'ftp[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    t = re.sub(pattern,' ',t) #remove ftp urls if any\n",
    "    t = re.sub(\"[\\w]+@[\\w]+\\.[c][o][m]\", \"\", t) #remove emails\n",
    "    t = cleanTags(t)\n",
    "    t = unescape(unidecode(t.lower().strip()))\n",
    "    t = t.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    #listKata = [tok for tok in TextBlob(t).words if (str(tok) not in stopWordsID)]\n",
    "    pattern = r\"[^a-zA-Z0-9 .,_-]\"\n",
    "    t = re.sub(pattern, \" \", t) # remove all Symbols except : {, . _ -}\n",
    "    t = re.sub(r'\\s+', ' ', t) # remove multiple spaces\n",
    "    t = \" \".join([w for w in t.split() if len(w)>=minWord and len(w)<=maxWord])\n",
    "    return t.strip()\n",
    "\n",
    "def getContents(dStr):\n",
    "    judul, pageID, body = None, None, None\n",
    "    if len(dStr)>7:\n",
    "        awalTag=re.search('<text.*?>',dStr).group(0)\n",
    "        awal=dStr.find(awalTag);akhir=dStr.find('</text>')\n",
    "        if awal>=0 and akhir>=0:\n",
    "            body=dStr[awal+len(awalTag):akhir]\n",
    "        dStr=dStr[:awal] # id and title came before <text>, no need to search the whole text again\n",
    "    if len(dStr)>7:\n",
    "        awal=dStr.find('<title>');akhir=dStr.find('</title>')\n",
    "        if awal>=0 and akhir>=0:\n",
    "            judul=dStr[awal+len('<title>'):akhir]\n",
    "        awal=dStr.find('<id>');akhir=dStr.find('</id>')\n",
    "        if awal>=0 and akhir>=0:\n",
    "            pageID=dStr[awal+len('<id>'):akhir]\n",
    "    return judul,pageID,body\n",
    "\n",
    "def getCategories(body):#if categories === if categories is not empty\n",
    "    categories=[];dTag1='[[Category:';dTag2=']]';n1=len(dTag1);n2=len(dTag2);\n",
    "    if type(body) is str:\n",
    "        while True and len(body)>12:\n",
    "            awal=body.find(dTag1);akhir=body.find(dTag2)\n",
    "            if awal>=0 and akhir>=0:\n",
    "                cat=body[awal+n1:akhir]\n",
    "                if len(cat)>1:\n",
    "                    categories.append(cat)\n",
    "                body=body[akhir+n2:]\n",
    "            else:\n",
    "                break\n",
    "    return ','.join(categories)\n",
    "\n",
    "# Buat pasangan konteks dan target\n",
    "def generate_context_target_pairs(tokens, context_size):\n",
    "    pairs = []\n",
    "    for token_list in tqdm(tokens):\n",
    "        token_list = [word for word in token_list if word in word_to_idx]\n",
    "        for i, word in enumerate(token_list):\n",
    "            target = word_to_idx[word]\n",
    "            context = [word_to_idx[token_list[i + j]] for j in range(-context_size, context_size + 1) if 0 <= i + j < len(token_list) and j != 0]\n",
    "            pairs.extend((context_word, target) for context_word in context)\n",
    "    return pairs\n",
    "\n",
    "# Dataset dan DataLoader\n",
    "class Word2VecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "    \n",
    "def get_word_embedding(word, GPU=True):\n",
    "    idx = word_to_idx.get(word)\n",
    "    if idx is not None:\n",
    "        with torch.no_grad():\n",
    "            if GPU:\n",
    "                embedding = model.embeddings(torch.tensor([idx]).cuda())\n",
    "            else:\n",
    "                embedding = model.embeddings(torch.tensor([idx]))\n",
    "        return torch.flatten(target_embedding).cpu()\n",
    "    else:\n",
    "        print(f\"Word '{word}' not in vocabulary\")\n",
    "        return None\n",
    "    \n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "def find_most_similar(word, top_n=5, GPU=True):\n",
    "    target_embedding = get_word_embedding(word, GPU=GPU)\n",
    "    if target_embedding is None:\n",
    "        return\n",
    "\n",
    "    similarities = []\n",
    "    for other_word in tqdm(vocab):\n",
    "        if other_word != word:\n",
    "            other_embedding = get_word_embedding(other_word, GPU=GPU)\n",
    "            similarity = cosine_similarity(target_embedding, other_embedding)\n",
    "            similarities.append((other_word, similarity))\n",
    "    \n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Load Dataset Wikipedia Indonesia Latest Dump</font></center>\n",
    "\n",
    "* Keterangan tentang wiki dumps: https://en.wikipedia.org/wiki/Wikipedia:Database_download\n",
    "* Wikipedia Indonesia dumps: https://id.wikipedia.org/wiki/Wikipedia:Unduh_basis_data\n",
    "\n",
    "<img alt=\"\" src=\"images/Wiki_logo.jpeg\" style=\"height: 400px;\" />\n",
    "\n",
    "## <center><font color=\"red\">Hati-hati akan mendownload data Text yang cukup besar (Hampir 1Gb pada Juli 2024)</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the previously parsed Wikipedia XML dump: data/idwiki-latest-pages-articles.zip\n",
      "(1550803, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>asam deoksiribonukleat</td>\n",
       "      <td>berkas dna structure key labelled.pn nobb.stru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>asam deoksiribosanukleat</td>\n",
       "      <td>alih asam deoksiribonukleat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anwar sadat</td>\n",
       "      <td>infobox officeholder native_name image anwar s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>azhar mansor</td>\n",
       "      <td>berkas mansor sedang mengemudikan kapal layar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>arkeologi</td>\n",
       "      <td>berkas bulgandry aboriginal site.situs arkeolo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_ category                     title  \\\n",
       "0    0      NaN    asam deoksiribonukleat   \n",
       "1    1      NaN  asam deoksiribosanukleat   \n",
       "2    2      NaN               anwar sadat   \n",
       "3    3      NaN              azhar mansor   \n",
       "4    4      NaN                 arkeologi   \n",
       "\n",
       "                                             content  \n",
       "0  berkas dna structure key labelled.pn nobb.stru...  \n",
       "1                        alih asam deoksiribonukleat  \n",
       "2  infobox officeholder native_name image anwar s...  \n",
       "3  berkas mansor sedang mengemudikan kapal layar ...  \n",
       "4  berkas bulgandry aboriginal site.situs arkeolo...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unduh data Wikipedia bahasa Indonesia\n",
    "url = \"https://dumps.wikimedia.org/idwiki/latest/idwiki-latest-pages-articles.xml.bz2\"\n",
    "dataFile = \"data/idwiki-latest-pages-articles.xml.bz2\"\n",
    "\n",
    "if not os.path.isfile(dataFile) and not os.path.isfile(dataFile.replace(\".xml.bz2\", \".zip\")):\n",
    "    print(\"Start Downloading Wikipedia Dumps Data : '{}' ...\".format(dataFile), flush=True, end=\" \")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(dataFile, \"wb\") as file:\n",
    "        for chunk in tqdm(response.iter_content(chunk_size=1024)):\n",
    "            file.write(chunk)\n",
    "            \n",
    "if os.path.isfile(dataFile.replace(\".xml.bz2\", \".zip\")):\n",
    "    print(\"Reading the previously parsed Wikipedia XML dump: {}\".format(dataFile.replace(\".xml.bz2\", \".zip\")))\n",
    "    df = pd.read_csv(dataFile.replace(\".xml.bz2\", \".zip\"), compression='zip')\n",
    "else:\n",
    "    print(\"Reading the Wikipedia XML dump: {}\".format(dataFile))\n",
    "    print(\"Please wait, it might take awhile .... \")\n",
    "    id_,  dPage = 0, []\n",
    "    df = {\"id_\":[], \"category\":[], \"title\":[], \"content\":[]}\n",
    "    with bz2.BZ2File(dataFile, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            line=line.decode('UTF-8')\n",
    "            if '<page>' in line:\n",
    "                dPage=[]\n",
    "            elif '</page>' in line:\n",
    "                try:\n",
    "                    judul, _, body = getContents(' '.join(dPage))\n",
    "                    cat = cleanText(getCategories(body))\n",
    "                    judul = cleanText(judul)\n",
    "                    body = cleanText(body)\n",
    "                    if len(str(judul))>5 and len(str(body))>5:\n",
    "                        df['id_'].append(id_); id_ += 1\n",
    "                        df['category'].append(cat)\n",
    "                        df['title'].append(judul)\n",
    "                        df['content'].append(body)\n",
    "                except Exception as err_:\n",
    "                    #print(err_)\n",
    "                    pass\n",
    "            else:\n",
    "                dPage.append(line)\n",
    "    df = pd.DataFrame(df)\n",
    "    print(\"Finished, Compressing the dataFrame for future operations ... \")\n",
    "    df.to_csv(dataFile.replace(\".xml.bz2\", \".zip\"), index=False, compression={'method':'zip', 'compresslevel':9})\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling 100000 documents.\n",
      "Joining Text ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100000it [00:00, 267906.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'foulques iv, comte anjou alih foulques iv dari anjou'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sampling {} documents.\".format(nSample))\n",
    "df = df.sample(nSample)\n",
    "\n",
    "print(\"Joining Text ... \")\n",
    "texts = [\" \".join([t, c]) for t,c in tqdm(zip(df.title, df.content))]\n",
    "del df # Free some memory\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 100000/100000 [01:28<00:00, 1129.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('\\nToken Length = 100000\\n',\n",
       " ['foulques', 'iv', ',', 'comte', 'anjou', 'alih', 'foulques'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenisasi teks\n",
    "print(\"Tokenizing ... \")\n",
    "tokens = [word_tokenize(text) for text in tqdm(texts)]\n",
    "del texts # Free Some memory\n",
    "\"\\nToken Length = {}\\n\".format(len(tokens)), tokens[0][:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Menyiapkan Struktur Data</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 100000/100000 [00:00<00:00, 161561.17it/s]\n",
      "139061it [00:00, 2601778.53it/s]\n",
      "139061it [00:00, 2834998.61it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 100000/100000 [00:35<00:00, 2786.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['foulques', 'comte', 'anjou', 'alih', 'dari']\n"
     ]
    }
   ],
   "source": [
    "# Buat korpus kata\n",
    "all_tokens = [token for sublist in tqdm(tokens) for token in sublist]\n",
    "vocab = Counter(all_tokens)\n",
    "vocab = [word for word, freq in vocab.items() if freq>=5 and len(word)>2]  # Filter kata yang jarang muncul\n",
    "\n",
    "# Peta kata ke indeks dan sebaliknya\n",
    "word_to_idx = {word: i for i, word in tqdm(enumerate(vocab))}\n",
    "idx_to_word = {i: word for i, word in tqdm(enumerate(vocab))}\n",
    "\n",
    "context_size = 2\n",
    "pairs = generate_context_target_pairs(tokens, context_size)\n",
    "\n",
    "dataset = Word2VecDataset(pairs)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(vocab[:5])\n",
    "del all_tokens, tokens # free some memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Model Word2Vec</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context_word):\n",
    "        embeds = self.embeddings(context_word)\n",
    "        out = self.linear1(embeds)\n",
    "        return out\n",
    "\n",
    "embedding_dim = 50\n",
    "if device.type.lower().strip()==\"cpu\":\n",
    "    model = Word2Vec(vocab_size, embedding_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    model = Word2Vec(vocab_size, embedding_dim).cuda()\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Training</font></center>\n",
    "\n",
    "### Tambah epoch di kasus nyata dan gunakan early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model, please wait, it will take a while .. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████                                                                | 1/5 [59:01<3:56:04, 3541.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9.765443939003308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▏                                              | 2/5 [2:05:44<3:10:39, 3813.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 8.91130614006109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████▊                               | 3/5 [3:08:47<2:06:38, 3799.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 8.651958212824683\n"
     ]
    }
   ],
   "source": [
    "nEpoch = 5 # Change to higher value in realworld scenario\n",
    "fmodel = 'data/models/word2Vec-wikiID.pth'\n",
    "try:\n",
    "    model = torch.load(fmodel)\n",
    "    print(\"Model Loaded from previous Run.\")\n",
    "except:\n",
    "    print(\"Training the model, please wait, it will take a while .. \")\n",
    "    for epoch in tqdm(range(nEpoch)):\n",
    "        total_loss = 0\n",
    "        for context_word, target in dataloader:\n",
    "            if device.type.lower().strip()==\"cpu\":\n",
    "                context_word, target = torch.tensor(context_word), torch.tensor(target)\n",
    "            else:\n",
    "                context_word, target = torch.tensor(context_word).cuda(), torch.tensor(target).cuda()\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(context_word)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
    "    print(\"Finished!, Saving The Model... \")\n",
    "    torch.save(model, fmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color=\"blue\">Contoh Penggunaan</font></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type.lower().strip()==\"cpu\":\n",
    "    GPU = False\n",
    "else:\n",
    "    GPU = True\n",
    "word = 'agama'\n",
    "most_similar_words = find_most_similar(word, top_n=7, GPU=True)\n",
    "print(f\"Kata-kata yang paling mirip dengan '{word}':\")\n",
    "for similar_word, similarity in most_similar_words:\n",
    "    print(f\"{similar_word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> Akhir Modul </font></center>\n",
    "\n",
    "<hr />\n",
    "<img alt=\"\" src=\"images/meme-cartoon/oprah-meme-everything-gets-a-vector-word-embedding.png\" style=\"height: 400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
