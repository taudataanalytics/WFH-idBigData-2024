{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2><strong><font color=\"blue\">WFH 2024 idBigData - Evaluasi LLM</font></strong></h2></center>\n",
    "\n",
    "<img alt=\"\" src=\"images/covers/cover_taudata_uin.jpg\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jangan lupa mengganti Runtime menjadi GPU di Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><strong><font color=\"blue\">Contoh 01: Akurasi</font></strong></h2></center>\n",
    "\n",
    "* small public LLM, like DistilBERT from Hugging Face’s Transformers library, on a text classification task.\n",
    "* the IMDb movie review dataset for binary sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore') \n",
    "\n",
    "try:\n",
    "    import google.colab; IN_COLAB = True\n",
    "    print(\"Installing the required modules\")\n",
    "    !pip install transformers peft datasets --q\n",
    "    print(\"preparing directories and assets\")\n",
    "    !mkdir data images output models\n",
    "    #!wget https://raw.githubusercontent.com/taudata...\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running the code locally, please make sure all the python module versions agree with colab environment and all data/assets downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small pre-trained model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"  # Fine-tuned on sentiment analysis\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load IMDb dataset with only 100 samples for quick evaluation\n",
    "dataset = load_dataset(\"imdb\", split=\"test[:100]\")  # Using a subset for demonstration\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Akurasi LLM pada kasus Sentimen Analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a pipeline for sentiment analysis\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Prepare predictions and labels\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "\n",
    "for example in dataset:\n",
    "    # Perform inference\n",
    "    result = sentiment_analyzer(example[\"text\"])[0]\n",
    "    pred_label = 1 if result[\"label\"] == \"POSITIVE\" else 0  # Map to binary label\n",
    "    true_label = example[\"label\"]  # IMDb dataset uses 0 for negative, 1 for positive\n",
    "\n",
    "    # Append predictions and true labels\n",
    "    pred_labels.append(pred_label)\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><strong><font color=\"blue\">Contoh 02: BLEU / ROUGE</font></strong></h2></center>\n",
    "\n",
    "* BLEU and ROUGE scores, which are commonly used for tasks like machine translation, summarization, and text generation.\n",
    "* In this example, we’ll evaluate a model’s text summarization capability using the CNN/DailyMail dataset.\n",
    "* We’ll use Hugging Face’s transformers library to load a pre-trained summarization model and datasets library to load the dataset.\n",
    "* The evaluation metrics BLEU and ROUGE will be calculated using nltk and rouge_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained summarization model\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "\n",
    "# Load a sample dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:10]\")  # Using a small subset for quick evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Summary menggunakan metric BLEU / ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ROUGE scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Prepare evaluation metrics\n",
    "bleu_scores = []\n",
    "rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "\n",
    "# Evaluate each example\n",
    "for sample in dataset:\n",
    "    # Generate summary with the model\n",
    "    generated_summary = summarizer(sample[\"article\"], max_length=50, min_length=25, do_sample=False)[0][\"summary_text\"]\n",
    "    \n",
    "    # Reference summary from the dataset\n",
    "    reference_summary = sample[\"highlights\"]\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    reference_tokens = nltk.word_tokenize(reference_summary)\n",
    "    generated_tokens = nltk.word_tokenize(generated_summary)\n",
    "    bleu = sentence_bleu([reference_tokens], generated_tokens)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # Compute ROUGE scores\n",
    "    rouge_score = rouge.score(reference_summary, generated_summary)\n",
    "    for key in rouge_scores.keys():\n",
    "        rouge_scores[key].append(rouge_score[key].fmeasure)\n",
    "\n",
    "# Calculate average scores\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_rouge = {key: sum(values) / len(values) for key, values in rouge_scores.items()}\n",
    "\n",
    "# Print results\n",
    "print(f\"Average BLEU score: {average_bleu:.4f}\")\n",
    "print(\"Average ROUGE scores:\")\n",
    "for key, score in average_rouge.items():\n",
    "    print(f\"  {key}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2><strong><font color=\"blue\">Contoh 03: Perplexity/Log Loss</font></strong></h2></center>\n",
    "\n",
    "* Perplexity and log loss are common metrics for evaluating the quality of language models, especially for tasks like language modeling where the model predicts the next word or token.\n",
    "* In this example, we’ll use Hugging Face’s transformers library to load a pre-trained GPT-2 model and a small dataset to calculate these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained language model\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load a small text dataset for evaluation\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test[:5%]\")  # Using a small subset for demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Perplexity dan Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize variables to track log loss and token count\n",
    "total_log_loss = 0\n",
    "total_tokens = 0\n",
    "\n",
    "# Calculate log loss and perplexity\n",
    "for sample in dataset:\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(sample[\"text\"], return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    \n",
    "    # Shift labels to align with prediction\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
    "    \n",
    "    # Forward pass with no gradient calculation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        log_likelihood = outputs.loss  # Log loss\n",
    "\n",
    "    # Accumulate total log loss and token count\n",
    "    batch_log_loss = log_likelihood.item() * inputs[\"input_ids\"].size(1)  # Log loss for this batch\n",
    "    total_log_loss += batch_log_loss\n",
    "    total_tokens += inputs[\"input_ids\"].size(1)\n",
    "\n",
    "# Calculate average log loss\n",
    "average_log_loss = total_log_loss / total_tokens\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = math.exp(average_log_loss)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Log Loss: {average_log_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
